{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hgarg97/ML-Code-Challenges/blob/main/MLCode_MachineLearning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Machine Learning\n",
        "\n",
        "## https://www.deep-ml.com/problems"
      ],
      "metadata": {
        "id": "mr-bWzI-QVK5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 14. Linear Regression Using Normal Equation\n"
      ],
      "metadata": {
        "id": "LODc6PO6Qh96"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ug4y3RPU3Ifk"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "def linear_regression_normal_equation(X: list[list[float]], y: list[float]) -> list[float]:\n",
        "\t# Your code here, make sure to round\n",
        "\t# X = np.array([1, 1], [1, 2], [1, 3])\n",
        "\t# y = np.array([1, 2, 3])\n",
        "\n",
        "\t# theta = ((X_T.X)^-1).X_T.y\n",
        "\tX = np.array(X)\n",
        "\ty = np.array(y).reshape(-1,1)\n",
        "\tX_T = X.T\n",
        "\n",
        "\ttheta = np.linalg.inv(X_T.dot(X)).dot(X_T).dot(y)\n",
        "\n",
        "\treturn np.round(theta, 4).flatten()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 15. Linear Regression Using Gradient Descent\n"
      ],
      "metadata": {
        "id": "58AVcBOQZi9g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def linear_regression_gradient_descent(X: np.ndarray, y: np.ndarray, alpha: float, iterations: int) -> np.ndarray:\n",
        "\t# Your code here, make sure to round\n",
        "\tm, n = X.shape\n",
        "\ttheta = np.zeros((n, 1))\n",
        "\n",
        "\tfor _ in range(iterations):\n",
        "\t\tpredictions = X @ theta\n",
        "\t\terrors = predictions - y.reshape(-1, 1)\n",
        "\t\tupdates = X.T @ errors / m\n",
        "\t\ttheta -= alpha*updates\n",
        "\treturn np.round(theta.flatten(), 4)"
      ],
      "metadata": {
        "id": "RA1DTcTjZkqk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 16. Feature Scaling Implementation"
      ],
      "metadata": {
        "id": "xYXMLheiRB6r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def feature_scaling(data: np.ndarray) -> (np.ndarray, np.ndarray):\n",
        "\n",
        "\t# Standardization\n",
        "\n",
        "\tmean = np.mean(data, axis = 0)\n",
        "\tstd = np.std(data, axis = 0)\n",
        "\n",
        "\tstandardized_data = (data-mean) / std\n",
        "\n",
        "\n",
        "\t# Normalization\n",
        "\n",
        "\tmin_val = np.min(data, axis = 0)\n",
        "\tmax_val = np.max(data, axis = 0)\n",
        "\n",
        "\tnormalized_data = (data-min_val) / (max_val - min_val)\n",
        "\n",
        "\treturn np.round(standardized_data, 4), np.round(normalized_data, 4)"
      ],
      "metadata": {
        "id": "iVGB80KdRERb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 17. K-Means Clustering"
      ],
      "metadata": {
        "id": "G-1S2PShVXr6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Works for 2-D Points\n",
        "\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "\n",
        "def euclidean_dist(a, b):\n",
        "\treturn np.sqrt((a[0]-b[0])**2 + (a[1]-b[1])**2)\n",
        "\n",
        "def k_means_clustering(points: list[tuple[float, float]], k: int, initial_centroids: list[tuple[float, float]], max_iterations: int) -> list[tuple[float, float]]:\n",
        "\n",
        "\tcentroids = initial_centroids.copy()\n",
        "\n",
        "\tfor iteration in range(max_iterations):\n",
        "\t\tclusters = defaultdict(list)\n",
        "\n",
        "\t\tfor point in points:\n",
        "\t\t\tdistances = [euclidean_dist(point, centroid) for centroid in centroids]\n",
        "\t\t\tclosest_centroid = distances.index(min(distances))\n",
        "\t\t\tclusters[closest_centroid].append(point)\n",
        "\n",
        "\t\tnew_centroids = []\n",
        "\n",
        "\t\tfor i in range(k):\n",
        "\t\t\tif clusters[i]:\n",
        "\t\t\t\tsum_x = sum(point[0] for point in clusters[i])\n",
        "\t\t\t\tsum_y = sum(point[1] for point in clusters[i])\n",
        "\t\t\t\tlen_clusters = len(clusters[i])\n",
        "\t\t\t\tnew_centroid = ((sum_x / len_clusters), (sum_y / len_clusters))\n",
        "\n",
        "\t\t\telse:\n",
        "\t\t\t\tnew_centroid = centroids[i]\n",
        "\n",
        "\t\t\tnew_centroids.append(new_centroid)\n",
        "\n",
        "\t\tif new_centroids == centroids:\n",
        "\t\t\treturn centroids\n",
        "\n",
        "\n",
        "\t\treturn new_centroids"
      ],
      "metadata": {
        "id": "qlwPgqawQni6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Works for all dimensions of tuples\n",
        "\n",
        "# Works for 2-D Points\n",
        "\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "\n",
        "def euclidean_dist(a, b):\n",
        "\t#return np.sqrt((a[0]-b[0])**2 + (a[1]-b[1])**2)\n",
        "  \ta_arr = np.array(a)\n",
        "  \tb_arr = np.array(b)\n",
        "  \treturn np.linalg.norm(a_arr - b_arr)\n",
        "\n",
        "\n",
        "def k_means_clustering(points: list[tuple[float, float]], k: int, initial_centroids: list[tuple[float, float]], max_iterations: int) -> list[tuple[float, float]]:\n",
        "\n",
        "\tcentroids = initial_centroids.copy()\n",
        "\n",
        "\tfor iteration in range(max_iterations):\n",
        "\t\tclusters = defaultdict(list)\n",
        "\n",
        "\t\tfor point in points:\n",
        "\t\t\tdistances = [euclidean_dist(point, centroid) for centroid in centroids]\n",
        "\t\t\tclosest_centroid = distances.index(min(distances))\n",
        "\t\t\tclusters[closest_centroid].append(point)\n",
        "\n",
        "\t\tnew_centroids = []\n",
        "\n",
        "\t\tfor i in range(k):\n",
        "\t\t\tif clusters[i]:\n",
        "\t\t\t\t# sum_x = sum(point[0] for point in clusters[i])\n",
        "\t\t\t\t# sum_y = sum(point[1] for point in clusters[i])\n",
        "\t\t\t\t# len_clusters = len(clusters[i])\n",
        "\t\t\t\t# new_centroid = ((sum_x / len_clusters), (sum_y / len_clusters))\n",
        "\n",
        "\t\t\t\t# Determine the dimensionality of the point\n",
        "\t\t\t\tdim = len(clusters[i][0])\n",
        "\t\t\t\t# Compute the mean for each dimension\n",
        "\t\t\t\tnew_centroid = tuple(\n",
        "\t\t\t\t\tsum(point[d] for point in clusters[i]) / len(clusters[i])\n",
        "\t\t\t\t\tfor d in range(dim)\n",
        "\t\t\t\t)\n",
        "\n",
        "\t\t\telse:\n",
        "\t\t\t\tnew_centroid = centroids[i]\n",
        "\n",
        "\t\t\tnew_centroids.append(new_centroid)\n",
        "\n",
        "\t\tif new_centroids == centroids:\n",
        "\t\t\treturn centroids\n",
        "\n",
        "\n",
        "\t\treturn new_centroids"
      ],
      "metadata": {
        "id": "HQlosA6bVhru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 19. Pricipal Component Analysis (PCA)"
      ],
      "metadata": {
        "id": "xN_T1sKHiaAV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def pca(data: np.ndarray, k: int) -> np.ndarray:\n",
        "\t# STEPS\n",
        "\n",
        "\t# 1. Standardize the data\n",
        "\t# 2. Calculate the covariance matrix of (X, X)\n",
        "\t# 3. Calculate the eigenvalues and eigenvectors of this covariance matrix\n",
        "\t# 4. Sort the eigenvectors w.r.t. eigenvalues in decreasing order\n",
        "\t# 5. Get the first k eigenvectors\n",
        "\n",
        "\t# Step 1\n",
        "\tdata_standardized = (data - np.mean(data, axis = 0)) / np.std(data, axis=0)\n",
        "\n",
        "\t# Step 2 (Transposing because cov needs column oriented matrix)\n",
        "\tcov_mat = np.cov(data_standardized.T)\n",
        "\n",
        "\t# Step 3\n",
        "\teigenvalues, eigenvectors = np.linalg.eig(cov_mat)\n",
        "\n",
        "\t# Step 4\n",
        "\tidx = np.argsort(eigenvalues)[::-1]\n",
        "\teigenvalues_sorted = eigenvalues[idx]\n",
        "\teigenvectors_sorted = eigenvectors[:, idx]\n",
        "\n",
        "\t# Step 5\n",
        "\tprincipal_components = eigenvectors_sorted[:, :k]\n",
        "\n",
        "\treturn np.round(principal_components, 4)"
      ],
      "metadata": {
        "id": "zfOExs9vidX5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_BdkRKwoiejI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}